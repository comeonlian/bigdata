storm
--------------
	流计算，实时计算.

进程
------------
	nimbus			//master
	supervisor		//slave
	worker			//工作进程
	core			//ui
	logviewer		//日志

组件
----------------
	Spout			//龙头
	Bolt			//转接头,逻辑计算单元
	tuple			//元组,数据的基本单位。
	stream			//一序列tuple.
	task			//执行spout 或者bolt
	executor		//程序执行官.
	topolgy			//拓扑，等价于mapreduce.
	zookeeper		//


并发度设置
---------------
	1.worker
		conf.setNumWorkers(2);
	2.任务数
		//任务数等价于对象的个数。
		boltDesclare.setNumTasks(3)
		SpoutDesclare.setNumTasks(4)

	3.并行暗示
		builder.setSpout("wcspout", new WordCountSpout(),3);


CPU
--------------
	线程数 == cpu的内核数。



Collections.synchronizedMap(map);
-----------------------------------



分组策略
---------------
	1.shuffle
		随机分组.

	2.field分组
		安装指定filed的key进行hash处理，
		相同的field，一定进入到同一bolt.

		该分组容易产生数据倾斜问题，通过使用二次聚合避免此类问题。

	3.使用二次聚合避免倾斜。
		a)App入口类
		[App.java]
		/**
		 * App
		 */
		public class App {
			public static void main(String[] args) throws Exception {
				TopologyBuilder builder = new TopologyBuilder();
				//设置Spout
				builder.setSpout("wcspout", new WordCountSpout()).setNumTasks(2);
				//设置creator-Bolt
				builder.setBolt("split-bolt", new SplitBolt(),3).shuffleGrouping("wcspout").setNumTasks(3);
				//设置counter-Bolt
				builder.setBolt("counter-1", new CountBolt(),3).shuffleGrouping("split-bolt").setNumTasks(3);
				builder.setBolt("counter-2", new CountBolt(),2).fieldsGrouping("counter-1",new Fields("word")).setNumTasks(2);

				Config conf = new Config();
				conf.setNumWorkers(2);
				conf.setDebug(true);

				/**
				 * 本地模式storm
				 */
				LocalCluster cluster = new LocalCluster();
				cluster.submitTopology("wc", conf, builder.createTopology());
				//Thread.sleep(20000);
		//        StormSubmitter.submitTopology("wordcount", conf, builder.createTopology());
				//cluster.shutdown();

			}
		}


		b)聚合bolt
		[CountBolt.java]
		package com.it18zhang.stormdemo.group.shuffle;

		import com.it18zhang.stormdemo.util.Util;
		import org.apache.storm.task.OutputCollector;
		import org.apache.storm.task.TopologyContext;
		import org.apache.storm.topology.IRichBolt;
		import org.apache.storm.topology.OutputFieldsDeclarer;
		import org.apache.storm.tuple.Fields;
		import org.apache.storm.tuple.Tuple;
		import org.apache.storm.tuple.Values;

		import java.util.HashMap;
		import java.util.Map;

		/**
		 * countbolt，使用二次聚合，解决数据倾斜问题。
		 * 一次聚合和二次聚合使用field分组，完成数据的最终统计。
		 * 一次聚合和上次split工作使用
		 */
		public class CountBolt implements IRichBolt{

			private Map<String,Integer> map ;

			private TopologyContext context;
			private OutputCollector collector;

			private long lastEmitTime = 0 ;

			private long duration = 5000 ;

			public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
				this.context = context;
				this.collector = collector;
				map = new HashMap<String, Integer>();
			}

			public void execute(Tuple tuple) {
				//提取单词
				String word = tuple.getString(0);
				Util.sendToLocalhost(this, word);
				//提取单词个数
				Integer count = tuple.getInteger(1);
				if(!map.containsKey(word)){
					map.put(word, count);
				}
				else{
					map.put(word,map.get(word) + count);
				}
				//判断是否符合清分的条件
				long nowTime = System.currentTimeMillis() ;
				if ((nowTime - lastEmitTime) > duration) {
					for (Map.Entry<String, Integer> entry : map.entrySet()) {
						//向下一环节发送数据
						collector.emit(new Values(entry.getKey(), entry.getValue()));
					}
					//清空map
					map.clear();
					lastEmitTime = nowTime ;
				}
			}

			public void cleanup() {
				for(Map.Entry<String,Integer> entry : map.entrySet()){
					System.out.println(entry.getKey() + " : " + entry.getValue());
				}
			}

			public void declareOutputFields(OutputFieldsDeclarer declarer) {
				declarer.declare(new Fields("word","count"));

			}

			public Map<String, Object> getComponentConfiguration() {
				return null;
			}
		}

	3.all分组
		使用广播分组。
		builder.setBolt("split-bolt", new SplitBolt(),2).allGrouping("wcspout").setNumTasks(2);

	4.direct(特供)
		只发送给指定的一个bolt.

		//a.通过emitDirect()方法发送元组
		//可以通过context.getTaskToComponent()方法得到所有taskId和组件名的映射
		collector.emitDirect(taskId,new Values(line));
		
		//b.指定directGrouping方式。
		builder.setBolt("split-bolt", new SplitBolt(),2).directGrouping("wcspout").setNumTasks(2);

	5.global分组
		对目标target tasked进行排序，选择最小的taskId号进行发送tuple
		类似于direct,可以是特殊的direct分组。

	6.自定义分组
		a)自定义CustomStreamGrouping类
			/**
			 * 自定义分组
			 */
			public class MyGrouping implements CustomStreamGrouping {

				//接受目标任务的id集合
				private List<Integer> targetTasks ;

				public void prepare(WorkerTopologyContext context, GlobalStreamId stream, List<Integer> targetTasks) {
					this.targetTasks = targetTasks ;
				}

				public List<Integer> chooseTasks(int taskId, List<Object> values) {
					List<Integer> subTaskIds = new ArrayList<Integer>();
					for(int i = 0 ; i <= targetTasks.size() / 2 ; i ++){
						subTaskIds.add(targetTasks.get(i));
					}
					return subTaskIds;
				}
			}

		b)设置分组策略
			public class App {
				public static void main(String[] args) throws Exception {
					TopologyBuilder builder = new TopologyBuilder();
					//设置Spout
					builder.setSpout("wcspout", new WordCountSpout()).setNumTasks(2);
					//设置creator-Bolt
					builder.setBolt("split-bolt", new SplitBolt(),4).customGrouping("wcspout",new MyGrouping()).setNumTasks(4);

					Config conf = new Config();
					conf.setNumWorkers(2);
					conf.setDebug(true);

					/**
					 * 本地模式storm
					 */
					LocalCluster cluster = new LocalCluster();
					cluster.submitTopology("wc", conf, builder.createTopology());
					System.out.println("hello world");
				}
			}





修改storm log输出
--------------------
	storm-core-1.3.jar下。
	[main/resources目录下]
	<configuration monitorInterval="60">
		<Appenders>
			<Console name="Console" target="SYSTEM_OUT">
				<PatternLayout pattern="%-4r [%t] %-5p %c{1.} - %msg%n"/>
			</Console>
		</Appenders>
		<Loggers>
			<Logger name="org.apache.zookeeper" level="ERROR"/>
			<Root level="error">
				<AppenderRef ref="Console"/>
			</Root>
		</Loggers>
	</configuration>


storm确保消息如何被完成处理
-----------------------------
	1.发送的tuple需要携带msgId
		collector.emit(new Values(line),index);

	2.bolt中需要对tuple进行确认(ack() | fail())
		public void execute(Tuple tuple) {
			String line = tuple.getString(0);
			System.out.println(this + " : " + line);
			if(new Random().nextBoolean()){
				//确认
				collector.ack(tuple);
			}
			else{
				//失败
				collector.fail(tuple);
			}
		}

	3.实现spout的ack()和fail()方法
		public void ack(Object msgId) {
			System.out.println(this + " : ack() : " + msgId);
		}

		public void fail(Object msgId) {
			System.out.println(this + " : fail() : " + msgId);
		}


kafka + storm
-----------------
	1.描述·
		storm以消费者从kafka队列中提取消息。

	2.添加storm-kafka依赖项
		[pom.xml]
		<?xml version="1.0" encoding="UTF-8"?>
		<project xmlns="http://maven.apache.org/POM/4.0.0"
				 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
				 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
			<modelVersion>4.0.0</modelVersion>

			<groupId>com.it18zhang</groupId>
			<artifactId>StormDemo</artifactId>
			<version>1.0-SNAPSHOT</version>

			<dependencies>
				<dependency>
					<groupId>org.apache.storm</groupId>
					<artifactId>storm-core</artifactId>
					<version>1.0.3</version>
				</dependency>
				<dependency>
					<groupId>junit</groupId>
					<artifactId>junit</artifactId>
					<version>4.11</version>
				</dependency>
				<dependency>
					<groupId>org.apache.storm</groupId>
					<artifactId>storm-kafka</artifactId>
					<version>1.0.2</version>
				</dependency>
				<dependency>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
					<version>1.2.17</version>
				</dependency>
				<dependency>
					<groupId>org.apache.kafka</groupId>
					<artifactId>kafka_2.10</artifactId>
					<version>0.8.1.1</version>
					<exclusions>
						<exclusion>
							<groupId>org.apache.zookeeper</groupId>
							<artifactId>zookeeper</artifactId>
						</exclusion>
						<exclusion>
							<groupId>log4j</groupId>
							<artifactId>log4j</artifactId>
						</exclusion>
					</exclusions>
				</dependency>
			</dependencies>

		</project>

	3.启动kafka + storm集群.
		public class App {
			public static void main(String[] args) throws Exception {
				TopologyBuilder builder = new TopologyBuilder();

				//zk连接串
				String zkConnString = "s202:2181" ;
				//
				BrokerHosts hosts = new ZkHosts(zkConnString);
				//Spout配置
				SpoutConfig spoutConfig = new SpoutConfig(hosts, "test2", "/test2", UUID.randomUUID().toString());
				spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
				KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

				builder.setSpout("kafkaspout", kafkaSpout).setNumTasks(2);
				builder.setBolt("split-bolt", new SplitBolt(),2).shuffleGrouping("kafkaspout").setNumTasks(2);

				Config conf = new Config();
				conf.setNumWorkers(2);
				conf.setDebug(true);

				/**
				 * 本地模式storm
				 */
				LocalCluster cluster = new LocalCluster();
				cluster.submitTopology("wc", conf, builder.createTopology());
			}
		}
	4.启动kafka集群和storm,使用生产者发送消息给kafka。
	
	5.看storm是否消费到。


storm整合hbase
---------------
	1.描述
		将计算结果写入到hbase数据库中。
		hbase 高吞吐量
		随机定位
		实时读写。
		
		master
		regionServer | region | wal | hadoop 

	2.创建hbase wordcount表,f1
		$>hbase shell
		$hbase shell>create 'ns1:wordcount' , 'f1'

	3.添加pom.xml
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-hbase</artifactId>
            <version>1.0.3</version>
        </dependency>
	4.HbaseBolt
		package com.it18zhang.stormdemo.hbase;

		import org.apache.storm.Config;
		import org.apache.storm.LocalCluster;
		import org.apache.storm.hbase.bolt.HBaseBolt;
		import org.apache.storm.hbase.bolt.mapper.SimpleHBaseMapper;
		import org.apache.storm.topology.TopologyBuilder;
		import org.apache.storm.tuple.Fields;

		/**
		 * App
		 */
		public class App {
			private static final String WORD_SPOUT = "WORD_SPOUT";
			private static final String COUNT_BOLT = "COUNT_BOLT";
			private static final String HBASE_BOLT = "HBASE_BOLT";

			public static void main(String[] args) throws Exception {
				//hbase映射
				SimpleHBaseMapper mapper = new SimpleHBaseMapper()
						.withRowKeyField("word")                //rowkey
						.withColumnFields(new Fields("word"))   //column
						.withCounterFields(new Fields("count")) //column
						.withColumnFamily("f1");                //列族

				HBaseBolt hbaseBolt = new HBaseBolt("ns1:wordcount", mapper);

				TopologyBuilder builder = new TopologyBuilder();
				builder.setSpout("wcspout", new WordCountSpout()).setNumTasks(1);
				builder.setBolt("split-bolt", new SplitBolt(),2).shuffleGrouping("wcspout").setNumTasks(2);
				builder.setBolt("hbase-bolt", hbaseBolt,2).fieldsGrouping("split-bolt",new Fields("word")).setNumTasks(2);

				Config conf = new Config();
				LocalCluster cluster = new LocalCluster();
				cluster.submitTopology("wc", conf, builder.createTopology());

			}
		}
